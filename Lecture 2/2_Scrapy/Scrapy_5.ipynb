{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Selectors ðŸ‘‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Scrapy ```Response``` object is a crucial component in the Scrapy framework, representing the HTTP response received after making a request. It encapsulates the content of the page, along with various attributes and methods that facilitate data extraction and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Attributes of the Response Object\n",
    "\n",
    "\n",
    "* url: The URL of the response. This is particularly useful for logging and debugging purposes.\n",
    "*    status: The HTTP status code of the response, which can help determine if the request was successful (e.g., 200) or if there were issues (e.g., 404, 500).\n",
    "*    body: The raw bytes of the response content. This can be used to access the content directly, especially when dealing with binary data.\n",
    "*    text: A convenience property that decodes the response body to a string using the response's encoding.\n",
    "*    headers: A dictionary-like object containing the response headers, which can provide additional context about the response.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Methods\n",
    "\n",
    "The Response object also provides several methods that enhance its functionality:\n",
    "\n",
    "*    ```css()```: This method allows you to select elements from the response using CSS selectors. It returns a SelectorList, which can be further processed to extract data.\n",
    "*    ```xpath()```: Similar to ```css()```, this method uses XPath expressions to select elements from the response. This is particularly useful for XML or HTML documents where XPath is preferred.\n",
    "*    ```json()```: If the response content is in JSON format, this method can be used to parse the JSON and return it as a Python dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create an ```HtmlResponse``` object by passing the URL and the HTML content as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "url = 'http://example.com'\n",
    "html_content = '<html><body><h1>Hello, World!</h1></body></html>'\n",
    "response = HtmlResponse(url=url, body=html_content, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet initializes an HtmlResponse object with the specified URL and HTML body. \n",
    "The encoding parameter ensures that the response is correctly interpreted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have an ```HtmlResponse``` object, you can access the response body using the ```.body``` attribute. This attribute contains the raw HTML content, which can be useful for debugging or logging purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<html><body><h1>Hello, World!</h1></body></html>'\n"
     ]
    }
   ],
   "source": [
    "print(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```HtmlResponse``` object provides a ```.selector``` attribute, which is an instance of ```scrapy.Selector```. This allows you to use **XPath** or **CSS** selectors to extract data from the HTML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Using XPath\n",
    "\n",
    "To extract the text from the \\<h1\\> tag, you can use the following ```XPath``` expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "h1_text = response.selector.xpath('//h1/text()').get()\n",
    "print(h1_text)  # Output: Hello, World!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Using CSS\n",
    "\n",
    "Alternatively, you can achieve the same result using **CSS** selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "h1_text_css = response.css('h1::text').get()\n",
    "print(h1_text_css)  # Output: Hello, World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "h1_text_css = response.selector.css('h1::text').get()\n",
    "print(h1_text_css)  # Output: Hello, World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More selector examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" are used to create multiline strings or even comments\n",
    "\n",
    "\"\"\"this is a multiline \n",
    "comment. It is very helpful in writing \n",
    "leghthy function descriptions. The\n",
    "next example is a how to initialize\n",
    "a string with multiple lines.\"\"\"\n",
    "\n",
    "\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "\n",
    "<html>\n",
    "  <head>\n",
    "    <base href='http://example.com/' />\n",
    "    <title>Example website</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <div id='images'>\n",
    "      <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' alt='image1'/></a>\n",
    "      <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' alt='image2'/></a>\n",
    "      <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' alt='image3'/></a>\n",
    "      <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' alt='image4'/></a>\n",
    "      <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' alt='image5'/></a>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by looking at the HTML code of that page, letâ€™s construct an XPath for selecting the text inside the title tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Selector query='//title/text()' data='Example website'>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://example.com'\n",
    "response = HtmlResponse(url=url, body=html_content, encoding='utf-8')\n",
    "response.xpath(\"//title/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "To actually extract the textual data, you must call the selector ```.get()``` or ```.getall()``` methods, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Example website']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath(\"//title/text()\").getall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example website'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath(\"//title/text()\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```.get()``` always returns a single result; if there are several matches, content of a first match is returned; \n",
    "if there are no matches, None is returned. \n",
    "\n",
    "```.getall()``` returns a list with all results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that CSS selectors can select text or attribute nodes using CSS3 pseudo-elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example website'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css(\"title::text\").get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, ```.xpath()``` and ```.css()``` methods return a ```SelectorList``` instance, which is a list of new selectors. \n",
    "This API can be used for quickly selecting nested data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1_thumb.jpg',\n",
       " 'image2_thumb.jpg',\n",
       " 'image3_thumb.jpg',\n",
       " 'image4_thumb.jpg',\n",
       " 'image5_thumb.jpg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css(\"img\").xpath(\"@src\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to extract only the first matched element, you can call the selector ```.get()``` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: My image 1 '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//div[@id=\"images\"]/a/text()').get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns ```None``` if no element was found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//div[@id=\"not-exists\"]/text()').get() is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using e.g. ```'@src'``` XPath it is possible to query for attributes using ```.attrib``` property of a Selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1_thumb.jpg',\n",
       " 'image2_thumb.jpg',\n",
       " 'image3_thumb.jpg',\n",
       " 'image4_thumb.jpg',\n",
       " 'image5_thumb.jpg']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[img.attrib[\"src\"] for img in response.css(\"img\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example.com/'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css(\"base\").attrib[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesting selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selection methods (```.xpath()``` or ```.css()```) return a list of selectors of the same type, so you can call the selection methods for those selectors too. Hereâ€™s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = response.xpath('//a[contains(@href, \"image\")]')\n",
    "\n",
    "links.getall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x10bf4f2c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(links) #The enumerate() function in Python adds a counter to an iterable, allowing for easy access to both index and element pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, <Selector query='//a[contains(@href, \"image\")]' data='<a href=\"image1.html\">Name: My image ...'>)\n",
      "(1, <Selector query='//a[contains(@href, \"image\")]' data='<a href=\"image2.html\">Name: My image ...'>)\n",
      "(2, <Selector query='//a[contains(@href, \"image\")]' data='<a href=\"image3.html\">Name: My image ...'>)\n",
      "(3, <Selector query='//a[contains(@href, \"image\")]' data='<a href=\"image4.html\">Name: My image ...'>)\n",
      "(4, <Selector query='//a[contains(@href, \"image\")]' data='<a href=\"image5.html\">Name: My image ...'>)\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(links):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, link in enumerate(links):\n",
    "    href_xpath = link.xpath(\"@href\").get()\n",
    "    img_xpath = link.xpath(\"img/@src\").get()\n",
    "    print(f\"Link number {index} points to url {href_xpath!r} and image {img_xpath!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting element attributesÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML attribtues\n",
    "\n",
    "HTML attributes provide additional information about HTML elements.\n",
    "\n",
    "\n",
    "*    All HTML elements can have attributes\n",
    "*    Attributes provide additional information about elements\n",
    "*    Attributes are always specified in the start tag\n",
    "*    Attributes usually come in name/value pairs like: name=\"value\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\<a\\> tag defines a hyperlink. The ```href``` attribute specifies the URL of the page the link goes to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <a href=\"https://www.w3schools.com\">Visit W3Schools</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to get a value of an attribute. First, one can use XPath syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath(\"//a/@href\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath syntax has a few advantages: it is a standard XPath feature, and ```@attributes``` can be used in other parts of an XPath expression - e.g. it is possible to filter by attribute value.\n",
    "\n",
    "Scrapy also provides an extension to CSS selectors (```::attr(...)```) which allows to get attribute values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.css(\"a::attr(href)\").getall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that, there is a ```.attrib``` property of Selector. You can use it if you prefer to lookup attributes in Python code, without using XPaths or CSS extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.attrib[\"href\"] for a in response.css(\"a\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using selectors with regular expressionsÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selector also has a ```.re()``` method for extracting data using regular expressions. \n",
    "However, unlike using ```.xpath()``` or ```.css()``` methods, ```.re()``` returns a list of strings. So you canâ€™t construct nested ```.re()``` calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s an example used to extract image names from the HTML code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My image 1 ', 'My image 2 ', 'My image 3 ', 'My image 4 ', 'My image 5 ']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//a[contains(@href, \"image\")]/text()').re(r\"Name:\\s*(.*)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereâ€™s an additional helper reciprocating ```.get()``` for ```.re()```, named ```.re_first()```. Use it to extract just the first matching string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My image 1 '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.xpath('//a[contains(@href, \"image\")]/text()').re_first(r\"Name:\\s*(.*)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage with Scrapy Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-16 15:57:13 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: scrapybot)\n",
      "2025-01-16 15:57:13 [scrapy.utils.log] INFO: Versions: lxml 5.2.2.0, libxml2 2.12.6, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.11.0, Python 3.10.4 (main, May 25 2024, 00:47:07) [Clang 15.0.0 (clang-1500.3.9.4)], pyOpenSSL 24.3.0 (OpenSSL 3.4.0 22 Oct 2024), cryptography 44.0.0, Platform macOS-15.2-arm64-arm-64bit\n",
      "2025-01-16 15:57:13 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-01-16 15:57:13 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2025-01-16 15:57:13 [scrapy.extensions.telnet] INFO: Telnet Password: d6919cd26c4428f6\n",
      "2025-01-16 15:57:13 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-01-16 15:57:13 [scrapy.crawler] INFO: Overridden settings:\n",
      "{}\n",
      "2025-01-16 15:57:13 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-01-16 15:57:13 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-01-16 15:57:13 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2025-01-16 15:57:13 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-01-16 15:57:13 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-01-16 15:57:13 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6042\n"
     ]
    },
    {
     "ename": "ReactorNotRestartable",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mReactorNotRestartable\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m process \u001b[38;5;241m=\u001b[39m CrawlerProcess() \u001b[38;5;66;03m#define the crawler\u001b[39;00m\n\u001b[1;32m     31\u001b[0m process\u001b[38;5;241m.\u001b[39mcrawl(QuotesSpider) \u001b[38;5;66;03m#attach the spider to the crawler\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/project1/lib/python3.10/site-packages/scrapy/crawler.py:496\u001b[0m, in \u001b[0;36mCrawlerProcess.start\u001b[0;34m(self, stop_after_crawl, install_signal_handlers)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m install_signal_handlers:\n\u001b[1;32m    493\u001b[0m     reactor\u001b[38;5;241m.\u001b[39maddSystemEventTrigger(\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartup\u001b[39m\u001b[38;5;124m\"\u001b[39m, install_shutdown_handlers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_shutdown\n\u001b[1;32m    495\u001b[0m     )\n\u001b[0;32m--> 496\u001b[0m \u001b[43mreactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstall_signal_handlers\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/project1/lib/python3.10/site-packages/twisted/internet/base.py:695\u001b[0m, in \u001b[0;36mReactorBase.run\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, installSignalHandlers: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartRunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstallSignalHandlers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmainLoop()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/project1/lib/python3.10/site-packages/twisted/internet/base.py:926\u001b[0m, in \u001b[0;36mReactorBase.startRunning\u001b[0;34m(self, installSignalHandlers)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorAlreadyRunning()\n\u001b[1;32m    925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startedBefore:\n\u001b[0;32m--> 926\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mReactorNotRestartable()\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signals\u001b[38;5;241m.\u001b[39muninstall()\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_installSignalHandlers \u001b[38;5;241m=\u001b[39m installSignalHandlers\n",
      "\u001b[0;31mReactorNotRestartable\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scrapy \n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\"https://quotes.toscrape.com/page/1/\"]\n",
    "\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'kaveri_QUOTES2.csv': { #3. where to save the extracted data\n",
    "                'format': 'csv',   #3. format of data. other formats like json and xml are also supported\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').get() \n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "process = CrawlerProcess() #define the crawler\n",
    "process.crawl(QuotesSpider) #attach the spider to the crawler\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the parse method uses the ```css()``` method of the ```Response``` object to extract the title of the page. The extracted title is then logged and yielded as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features of Scrapy Spiders\n",
    "\n",
    "*    **Asynchronous Processing**: Scrapy spiders operate asynchronously, allowing them to handle multiple requests simultaneously, which significantly speeds up the crawling process.\n",
    "*    **Selectors**: Scrapy provides powerful selectors based on XPath and CSS, enabling spiders to extract data from HTML documents efficiently.\n",
    "*    **Middleware**: You can customize the behavior of your spider by using middleware, which allows you to process requests and responses globally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Key Features\n",
    "\n",
    "*    Selectors: The HtmlResponse class allows you to use both XPath and CSS selectors to navigate and extract data from the HTML document.\n",
    "*    Convenience: The response object in Scrapy automatically provides access to the selector, making it easy to work with HTML content without manual instantiation.\n",
    "*    Encoding: Proper encoding is crucial for accurately processing the HTML content, especially when dealing with non-ASCII characters.\n",
    "\n",
    "By understanding how to create and utilize HtmlResponse objects, you can effectively scrape and manipulate HTML data in your Scrapy projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Writing Spiders\n",
    "\n",
    "*    Keep it Simple: Start with a simple spider and gradually add complexity as needed. This helps in debugging and maintaining the code.\n",
    "*    Respect ```Robots.txt```: Always check the robots.txt file of the website to ensure that your spider complies with the site's crawling policies.\n",
    "*    Use Item Loaders: For more complex data extraction, consider using item loaders to clean and validate the data before yielding it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Scrapy spiders are versatile and powerful tools for web scraping. By understanding their structure and capabilities, you can effectively extract data from various sources. For more detailed information, refer to the official Scrapy Spiders Documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
