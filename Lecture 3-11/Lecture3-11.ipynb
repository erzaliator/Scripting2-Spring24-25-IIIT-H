{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5bb4743",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Lecture Series from Stanford CS 124: https://www.youtube.com/watch?v=808M7q8QX0E&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&ab_channel=FromLanguagestoInformation\n",
    "\n",
    "Lecture 7 and 8 have separate references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b9cf5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8a3d44f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 3a. Revising Regex, Corpora and Tokenization\n",
    "\n",
    "## Source:\n",
    "\n",
    "1 3 Words and Corpora https://www.youtube.com/watch?v=xsIDTmo1NOg&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=3 </br>\n",
    "1 4 Word Tokenization https://www.youtube.com/watch?v=xsIDTmo1NOg&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=4 </br>\n",
    "1 6 Word Normalization https://www.youtube.com/watch?v=xsIDTmo1NOg&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=6\n",
    "\n",
    "# Lecture 3b. Language model\n",
    "\n",
    "## Source:\n",
    "3 1 https://www.youtube.com/watch?v=hM49MPmakNI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=12\n",
    "\n",
    "## Class Viva:\n",
    "Q. What is a language model?\n",
    "P(w1, w2, w3, w4) or P(w4|w1, w2, w3)\n",
    "\n",
    "Q. What is joint probability?\n",
    "P(A,B)\n",
    "\n",
    "Q. What is conditional probability?\n",
    "P(A|B)\n",
    "\n",
    "Q. Formula of cond prob.?\n",
    "P(A|B)=P(A,B)/P(B)\n",
    "P(A,B) = P(A|B)X P(B)\n",
    "\n",
    "Q. What is chain rule?\n",
    "P(A, B, C,D) = P(A|B,C,D)P(B|C,D)P(C|D)P(D)\n",
    "\n",
    "Q. Write prob formula of Markov assumption?\n",
    "P(Wn) = pi  < PLEASE WRITE FULL FORMULA IN ASSIGNMENT\n",
    "\n",
    "Q. What is formula of a unigram versus a bigram model?\n",
    "Wi Wi-1 Wi-2 …. Wi-n\n",
    "P(wi) = C(chepu)/total telugu - uni\n",
    "P(wi|wi-1) = P(parking|car) = P(car parking)/P(car) = 2/10 - bi\n",
    "P(wi|wi-1, wi-2) - tri\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81625688",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a029c06",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 4. Bigrams and MLE\n",
    "\n",
    "## Source:\n",
    "3 2 https://www.youtube.com/watch?v=UyC0bBiZY-A&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=13\n",
    "\n",
    "## Class Viva\n",
    "\n",
    "Q. MLEstimate of a bigram\n",
    " - P(wi|wi-1) = C(wi-1,wi)/C(wi-1)\n",
    "\n",
    "Q. Why do we store in log probs?\n",
    "- underflow: \n",
    "- formula for log: adding is faster than multiplying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b06aa96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc15236",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 5. Naive Bayes Classifier\n",
    "\n",
    "## Source:\n",
    "5 1, What is Text Classification? https://www.youtube.com/watch?v=Y1j_J53k7fo&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=19 </br>\n",
    "5 2, The Naive Bayes Classifier https://www.youtube.com/watch?v=OhLosjXM-Fg&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=20 </br>\n",
    "5 3, Learning in Naive Bayes https://www.youtube.com/watch?v=Ge612JZGBMU&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=21 </br>\n",
    "5 4, Sentiment and Binary Naive Bayes https://www.youtube.com/watch?v=tjDceq0qH10&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=22 </br>\n",
    "\n",
    "## Class Viva:\n",
    "\n",
    "Q. What is Bayes rule?\n",
    "A. P(a|b) = P(b|a)P(a)/P(b)\n",
    "\n",
    "cNB = argmax P(d|c)P(c)\n",
    "\n",
    "Q. What is a likelihood probability? What is the prior probability?\n",
    "A. P(d|c) and P(c)\n",
    "\n",
    "Q. How can we compute the prior probability in an author recognition task? Assume c belongs to {“RBT”, “MG”, “RK”, … 10 author names} \n",
    "Hint: Think about each author such Rabindranath Tagore, Mahatma Gandhi, Rudyard Kipling, etc. as a class.\n",
    "A. P(c=“RNT”) = count of RNT docs/ number of docs\n",
    "\n",
    "Q. How can we compute likelihood of a class=\"rabindranath tagore” in author recognition task? \n",
    "book = \"janagana mana adhinayaka .asdasdas”\n",
    "A. P(d|c=RBT)= 2/10000 * 3/10000 \n",
    "\n",
    "Q. What is naive bayes linear classifier?\n",
    "\n",
    "## Assignment:\n",
    "\n",
    "Read the following resources:\n",
    "* https://www.analyticsvidhya.com/blog/2022/03/building-naive-bayes-classifier-from-scratch-to-perform-sentiment-analysis/\n",
    "* https://www.kaggle.com/code/keithcooper/sentiment-analysis-with-naive-bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af88360",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b32ceba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 6. Naive Bayes Classifier and Tf-Idf Retrieval Model\n",
    "\n",
    "## Source:\n",
    "5 7 Precision, Recall, and the F measure https://www.youtube.com/watch?v=mjOgb4T7yyo&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=25 </br>\n",
    "7 1 Intro to IR https://www.youtube.com/watch?v=kNkCfaH2rxc&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=33 </br>\n",
    "7 2 Term Document Incidence Matrices https://www.youtube.com/watch?v=e81nC0LO0A8&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=34  </br>\n",
    "7 5 The Boolean Retrieval Model https://www.youtube.com/watch?v=TIN_02pJU-Y&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=37 </br>\n",
    "8 1 Introducing Ranked Retrieval https://www.youtube.com/watch?v=ZrNmCtSrL48&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=39\n",
    "\n",
    "Assignment\n",
    "8 3 Term Freq Weighing https://www.youtube.com/watch?v=9UXM2NXVYY0&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=41 </br>\n",
    "8 4 Inverse Document Frquency Weighing https://www.youtube.com/watch?v=7nWlI_TVid0&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=42 </br>\n",
    "8 5 TF IDF Weighing https://www.youtube.com/watch?v=4-P3ckZprBk&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=43 </br>\n",
    "\n",
    "## Class Viva:\n",
    "\n",
    "Q. What is precision and recall for Naive Bayes classifier?\n",
    "Precision=TP(correctly predicted)/TP+FP (all positive PREDICTED) = how many of the PREDICTED positives were correctly predicted by the model\n",
    "Recall = TP(correctly predicted)/TP+FN (all positive ground truths) = how many of the ACTUAL positives were predicted by the model\n",
    "\n",
    "Q. What is precision and recall for IR system?\n",
    "Precision = how many of the retrieved documents were correct = correct docs/retrieved docs\n",
    "Recall = how many of the actually relevant documents were retrieved = correct doc/docs needed\n",
    "\n",
    "Q. What is a query?\n",
    "It is a boolean function of all TERM values.\n",
    "\n",
    "Q. What is a TERM INCIDENCE matrix and what are the boolean values in it?\n",
    "Term matrix is matrix of the terms of the terms and docs that we use to retrieve the presences of the terms in a doc. Boolean values measure the presence/absence in a doc. And they populated.\n",
    "\n",
    "Q. What are the drawbacks of term document matrix? What is the workaround?\n",
    "A. Because the matrix is a NXM size, i.e. 500K words X 1M docs will be a 1B times big ~0.5Billion. Also, sparsity.\n",
    "\n",
    "Q. What is  a bag of words model? And what is a TERM FREQUENCY matrix?\n",
    "No differentiation between order of words. No positional information is a drawback.\n",
    "\n",
    "Q. What is a term frequency tf_t,d?\n",
    "\n",
    "Q. Why does frequency always does not mean relevance?\n",
    "Because a frequency of a word that is generally frequent over all documents does not tell us much. That is measured by idf - to penalize words (such as stop words, etc) with more frequencies i.e. to lift rare term.\n",
    "\n",
    "Q. Which of the two tf or idf does not change with the documents?\n",
    "\n",
    "Q. What is a collection frequency and how is it different from term frequency?\n",
    "\n",
    "## Assignment:\n",
    "\n",
    "### Read the following lectures:\n",
    "\n",
    "7 1 > 7:00 min mark, precision and recall\n",
    "7 2> Term document indicence matrices\n",
    "8 3> Term frequency\n",
    "8 5 > tfidf\n",
    "8 6\n",
    "8 7\n",
    "8 8\n",
    "\n",
    "### Solve the following problems:\n",
    "\n",
    "Q1 [5 marks]. a. You are making an information retrieval system which processed documents (single sentences) and returns the top 2 docs with highest tf-idf score based on a query.\n",
    "Implement the non scikit-learn computation of TF-IDF MATRIX for the given corpus = [‘there ‘are three cars parked on the road next to the water park,\n",
    "          ‘the parked car is three meters wide’,\n",
    "          ‘the car meters are always next to water’,\n",
    "          ‘three cars are parked by the meters’ ]\n",
    "Based on your implementation - What will be the RANKING of the documents based on the query - “three cars”? What top 2 documents will be returned by the system?\n",
    "b. [2 marks] What is the term frequency of ‘three’ for each of the four documents?\n",
    "c. [1 marks] What is the collection frequency of ’three’?\n",
    "d. [2 marks] What is the idf of ‘three’ in the corpus?\n",
    ": https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency/ \n",
    "\n",
    "Q2 [5 marks]. Why does our implementation not match with tf-idf using scikit learn?\n",
    "\n",
    "Q3. [0 marks] Read the solved tf-idf example with formulas: https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e9d070",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f72fb114",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 7. Vector space model\t\n",
    "\n",
    "## Source:\n",
    "8 6 Vector Space Model https://www.youtube.com/watch?v=o5nflzfX5tw&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=44 </br>\n",
    "8 7 Calculating TF IDF Cosine Scores https://www.youtube.com/watch?v=k1tD7pYKWuM&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=45 </br>\n",
    "8 8 Evaluating Search Engines https://www.youtube.com/watch?v=b7pfLpVBN84&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=46\n",
    "\n",
    "## Class Viva:\n",
    "\n",
    "Q. Why did we move from boolean model to bag of words models to vector space model?\n",
    "A. Incidences only\n",
    "Frequencies only\n",
    "\n",
    "\n",
    "Q. Should we use Euclidean distance to measure the distance between two document representations?\n",
    "Think about a document with two terms and document with 200 terms. So now think of a natural condition of:\n",
    "1. D1 has 2000 terms and D2 and 20000 terms. Both talk about Anthony. \n",
    "2. D1 has 2000 terms and D3 has 5000 terms. One talks about Anthony and the other talks about Brutus\n",
    "(Document length is of course of the many aspects that can increase of document length).\n",
    "So we use cosine distance\n",
    "\n",
    "Q. What is a document’s vector representation? What are the individual terms in it?\n",
    "\n",
    "Q. What is a cosine distance? \n",
    "\n",
    "Q. Advantages of vector space model over boolean model?\n",
    "Having more than boolean values allows us the ranking the results of the IR-sys.\n",
    "\n",
    "Q. how do you evaluate a boolean retrieval model?\n",
    "Precision, Recall\n",
    "\n",
    "Q. How do you evaluate a vector space model?\n",
    "1. *Humans* rate the relevance of the ranking\n",
    "2. Compute MAP\n",
    "\n",
    "Q. How to compute MAP?\n",
    "Compute precision at each value when a relevant document is returned.\n",
    "AP = average of all precisions (AP measures the quality of single query’s documents)\n",
    "MAP = mean of all APs of all queries (MAP measures the quality of all queries of the search engine)\n",
    "\n",
    "\n",
    "## Class readings:\n",
    "1. Vector space model important definitions: https://en.wikipedia.org/wiki/Vector_space_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc1235",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bace0f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 8. Vector Semantics\n",
    "\n",
    "## Source: \n",
    "V 1 Word Meaning https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=49 </br>\n",
    "V 2 Vector Semantics https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=50 </br>\n",
    "V 3 Words and Vectors https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=51 </br>\n",
    "V 4 Cosine Similarity https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=52 </br>\n",
    "V 5 TF IDF https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=53 </br>\n",
    "V 6 Word2Vec https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=54 </br>\n",
    "V 7 Learning in Word2Vec https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=55 </br>\n",
    "V 8 Properties of Embeddings https://www.youtube.com/watch?v=EsfNYiLVtHI&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=56\n",
    "\n",
    "## Class Viva:\n",
    "\n",
    "Q. What are antonyms? \n",
    "Opposite words\n",
    "\n",
    "Q. What are connotations?\n",
    "Related to word’s meanings and sentiments.\n",
    "Positive connotation - Antidote\n",
    "Negative connotation - Poison\n",
    "\n",
    "Q. How can you use SentiWordNet to measure the sentiment of a sentence? What about negation? What are the drawbacks of this method?\n",
    "Average out the sentiments of the words in a sentence. Normalize by sentence length (divide).\n",
    "Maintain a list of negative markers. For each negative marker, reverse the next word. \n",
    "It does not take into account longer phrases.\n",
    "It does not take into account some sentences are more negative than others. Terrible vs bad. Very bad vs bad.\n",
    "Informal languages - Ignores emojis :) :))))))) , repetitions (tooooooooooo good vs too good)\n",
    "\n",
    "Q. How can you use WordNet to get the meaning of word?\n",
    "Gloss\n",
    "\n",
    "Q. What are the drawbacks of tf-idf vector representations?\n",
    "1. Dimensionality is too high. Very long. What is dimensionality?\n",
    "2. They are sparse.\n",
    "\n",
    "Q. What are the two probabilities that a skip gram model predicts?\n",
    "1. P(-| word1, word2=context) = not occuring together\n",
    "2. P(+|word1, word2) = sigmoid(c.w) = 1 / 1+exp(c,w) = occurring together\n",
    "\n",
    "Q. What is a sense? \n",
    "A sense a discrete representation of one aspect of the meaning of a word. Bank1 and bank2 are two senses of the word bank.\n",
    "\n",
    "\n",
    "\n",
    "## Class readings:\n",
    "1. https://wordnetcode.princeton.edu/5papers.pdf\n",
    "2. https://www.youtube.com/watch?v=ZNze2VHgsrE&ab_channel=RobertPNLP  \n",
    "3. http://www.lrec-conf.org/proceedings/lrec2010/pdf/769_Paper.pdf \n",
    "4. https://radimrehurek.com/gensim/models/word2vec.html \n",
    "5. Read page 16 and 17 of this to see the second last vector lecture: https://arxiv.org/pdf/1411.2738 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cab837f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f01c9760",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 9. Logistic Regression\n",
    "\n",
    "LR 1 Generative and Discriminative classifiers https://www.youtube.com/watch?v=qZSqcu2D7zQ&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=27 </br>\n",
    "LR 2 Classification https://www.youtube.com/watch?v=qZSqcu2D7zQ&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=28 </br>\n",
    "LR 3 Sentiment Example https://www.youtube.com/watch?v=qZSqcu2D7zQ&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=29 </br>\n",
    "LR 4 Cross Entropy https://www.youtube.com/watch?v=qZSqcu2D7zQ&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=30 \n",
    "\n",
    "## Class Viva:\n",
    "\n",
    "Q. What is probability of a correct class c in a generative v discriminative classifier? \n",
    "A. c_hat = c argmax P(d|c) P(c)\n",
    "c_hat = c argmax P(c|d)\n",
    "\n",
    "Q. PLOT 3 activation functions? - out of syllabus. Please google.\n",
    "Tan, Relu, Sigmoid\n",
    "\n",
    "Q. What is the cross entropy loss function/ log loss?\n",
    "Ce Loss(y’, y) = -(ylog sig(wx+b_ + (1-y)log(1- sig(wx+b) )\n",
    "-inf, +inf\n",
    "0 , +1\n",
    "\n",
    "Q. Read this notebook and understand the implementation of logistic regression.\n",
    "https://github.com/llSourcell/logistic_regression/blob/master/Sentiment%20analysis%20with%20Logistic%20Regression.ipynb \n",
    "\n",
    "Q. Why are activation functions used?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e08a8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1733481",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 10. Neural Networks and RNNs\n",
    "\n",
    "## Source:\n",
    "N5 1 Neural Units https://www.youtube.com/watch?v=BtmsIy0j_dY&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=57 </br>\n",
    "N5 2 XOR https://www.youtube.com/watch?v=BtmsIy0j_dY&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=58 </br>\n",
    "N5 3 Feedforward https://www.youtube.com/watch?v=BtmsIy0j_dY&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=59 </br>\n",
    "N5 4 Applying Feedforward https://www.youtube.com/watch?v=BtmsIy0j_dY&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=60 </br>\n",
    "N5 5 Training https://www.youtube.com/watch?v=BtmsIy0j_dY&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=61 </br>\n",
    "N5 6 Computation Graphs https://www.youtube.com/watch?v=BtmsIy0j_dY&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=62 </br>\n",
    "* BPTT in RNNs: https://www.youtube.com/watch?v=I4_lyJVnpMg&t=334s&ab_channel=NPTEL-NOCIITM\n",
    "\n",
    "## Class Viva:\n",
    "Q. Read the following article\n",
    "https://www.youtube.com/watch?v=exaWOE8jvy8&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=2&ab_channel=PatrickLoeber \n",
    "https://www.youtube.com/watch?v=JHWqWIoac2I&ab_channel=Codemy.com \n",
    "https://www.youtube.com/watch?v=I4_lyJVnpMg&t=334s&pp=ygUIcm5uIGJwdHQ%3D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691d375",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86020e46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lecture 11. RNNs, Transformers and BERT\n",
    "\n",
    "## Source and further reading:\n",
    "\n",
    "Transformers: https://www.youtube.com/watch?v=4Bdc55j80l8&t=148s&ab_channel=TheAIHacker \n",
    "https://www.youtube.com/watch?v=TQQlZhbC5ps&ab_channel=CodeEmporium  \n",
    "BERT: https://www.youtube.com/watch?v=xI0HHN5XKDo&ab_channel=CodeEmporium \n",
    "T5: https://www.youtube.com/watch?v=91iLu6OOrwk \n",
    "\n",
    "\n",
    "Documentation: https://huggingface.co/docs/transformers/en/index\n",
    "https://huggingface.co/docs/transformers/en/model_doc/bert#resources \n",
    "\n",
    "Adapters with PEFT: https://huggingface.co/docs/transformers/en/peft \n",
    "\n",
    "\n",
    "Longer videos:\n",
    "1. https://www.youtube.com/watch?v=bCz4OMemCcA&ab_channel=UmarJamil \n",
    "2. https://datascience.stackexchange.com/questions/36862/macro-or-micro-average-for-imbalanced-class-problems \n",
    "3. https://www.kaggle.com/code/houssemayed/imdb-sentiment-classification-with-bert "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
